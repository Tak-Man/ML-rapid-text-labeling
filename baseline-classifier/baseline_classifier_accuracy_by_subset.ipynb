{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automatic-objective",
   "metadata": {},
   "source": [
    "## Master of Applied Data Science\n",
    "### University of Michigan - School of Information\n",
    "### Capstone Project - Rapid Labeling of Text Corpus Using Information Retrieval Techniques\n",
    "### Fall 2021\n",
    "#### Team Members: Carlo Tak, Michael Penrose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-exhibit",
   "metadata": {},
   "source": [
    "### Experiment Flow\n",
    "\n",
    "Class label > Count vectorizer > 800 features > scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-partition",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "\n",
    "This notebook investigates how well a classifier can predict the **event type (i.e. 'earthquake', 'fire', 'flood', 'hurricane)** of the Tweets in the [Disaster tweets dataset](https://crisisnlp.qcri.org/humaid_dataset.html#).\n",
    "\n",
    "This classifier is to be used as a baseline of classification performance. Two things are investigated:\n",
    "- Is it possible to build a reasonable 'good' classifier of these tweets at all\n",
    "- If it is possible to build a classifier how well does the classifier perform using all of the labels from the training data\n",
    "\n",
    "If it is possible to build a classifier using all of the labels in the training dataset then it should be possible to implement a method for rapidly labeling the corpus of texts in the dataset. Here we think of rapid labeling as any process that does not require the user to label each text in the corpus, one at a time.\n",
    "\n",
    "To measure the performance of the classifier we use a metric called the Area Under the Curve (AUC). This metric was used because we believe it is a good metric for the preliminary work in this project. If a specific goal emerges later that requires a different metric, then the appropriate metric can be used at that time. The consequence of false positives (texts classified as having a certain label, but are not that label) and false negatives should be considered. For example, a metric like precision can be used to minimize false positives. The AUC metric provides a value between zero and one, with a higher number indicating better classification performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-above",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The baseline classifier built using all the labels in the training dataset produced a classifier that had a fairly good AUC score for each of the 4 event type labels (i.e. earthquake, fire, flood, hurricane). All the AUC scores were above 0.98.\n",
    "\n",
    "A simple vectorization (of texts) approach was implemented because we wanted the baseline classifier to be a basic solution – our feeling was that more complex techniques could be implemented at a later stage. A [count vectorizer]( https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) (with default settings) was used to convert the texts. The number of dimensions (features) was also reduced using feature selection ([SelectKBest]( https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)). This was to improve computational times – fewer dimensions means that there are fewer data to process. Also, this was a simpler method to implement than other techniques like removing stopwords, adjusting parameters like ‘stop_words’, ‘ngram_range’, ‘max_df’, ‘min_df’, and ‘max_features’.  The complexity of the classifier could be adjusted if required, but this simple implementation produced good results.\n",
    "\n",
    "This notebook reduced the number of features to 100.\n",
    "\n",
    "The feature importances were extracted from the classifier, to see if they made sense. This sense check was important because we made several assumptions in building this classifier, that had to be validated. For example, when the text was vectorized we used a simple approach that just counted the individual words (tokens) – are more complex classifier might use bi-grams (two words per feature), this would have had the advantage of preserving features like ‘’.\n",
    "\n",
    "Examining the top features\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "empty-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import dt_utilities as utils\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Acceleration for scikit-learn on Windows 64 bit machines\n",
    "# from sklearnex import patch_sklearn\n",
    "# patch_sklearn()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC, NuSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "import scipy.sparse\n",
    "from collections import Counter\n",
    "import altair as alt\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "japanese-dollar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('default')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable correct rendering\n",
    "alt.renderers.enable('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "studied-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "start_time.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "RANDOM_STATE = 257"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-robertson",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "million-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_disaster_tweet_data_df = \\\n",
    "    utils.get_consolidated_disaster_tweet_data(root_directory=\"data/\",\n",
    "                                               event_type_directory=\"HumAID_data_event_type\",\n",
    "                                               events_set_directories=[\"HumAID_data_events_set1_47K\",\n",
    "                                                                       \"HumAID_data_events_set2_29K\"],\n",
    "                                               include_meta_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tracked-cisco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>class_label</th>\n",
       "      <th>event_type</th>\n",
       "      <th>data_type</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>798262465234542592</td>\n",
       "      <td>sympathy_and_support</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>dev</td>\n",
       "      <td>RT @MissEarth: New Zealand need our prayers af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>771464543796985856</td>\n",
       "      <td>caution_and_advice</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>dev</td>\n",
       "      <td>@johnaglass65 @gordonluke Ah, woke up to a nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>797835622471733248</td>\n",
       "      <td>requests_or_urgent_needs</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>dev</td>\n",
       "      <td>RT @terremotocentro: #eqnz if you need a tool ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>798021801540321280</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>dev</td>\n",
       "      <td>RT @BarristerNZ: My son (4) has drawn a pictur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>798727277794033664</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>dev</td>\n",
       "      <td>Due to earthquake damage our Defence Force is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                        class_label  event_type  \\\n",
       "0  798262465234542592               sympathy_and_support  earthquake   \n",
       "1  771464543796985856                 caution_and_advice  earthquake   \n",
       "2  797835622471733248           requests_or_urgent_needs  earthquake   \n",
       "3  798021801540321280         other_relevant_information  earthquake   \n",
       "4  798727277794033664  infrastructure_and_utility_damage  earthquake   \n",
       "\n",
       "  data_type                                         tweet_text  \n",
       "0       dev  RT @MissEarth: New Zealand need our prayers af...  \n",
       "1       dev  @johnaglass65 @gordonluke Ah, woke up to a nig...  \n",
       "2       dev  RT @terremotocentro: #eqnz if you need a tool ...  \n",
       "3       dev  RT @BarristerNZ: My son (4) has drawn a pictur...  \n",
       "4       dev  Due to earthquake damage our Defence Force is ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consolidated_disaster_tweet_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abstract-collector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>class_label</th>\n",
       "      <th>event_type</th>\n",
       "      <th>data_type</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>798064896545996801</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>train</td>\n",
       "      <td>I feel a little uneasy about the idea of work ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>797913886527602688</td>\n",
       "      <td>caution_and_advice</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>train</td>\n",
       "      <td>#eqnz Interislander ferry docking aborted afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>797867944546025472</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>train</td>\n",
       "      <td>Much of New Zealand felt the earthquake after ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>797958935126773760</td>\n",
       "      <td>sympathy_and_support</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>train</td>\n",
       "      <td>Noticing a lot of aftershocks on eqnz site, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>797813020567056386</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>train</td>\n",
       "      <td>RT @E2NZ: Mike Clements, NZ police, says obvio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                        class_label  event_type  \\\n",
       "0  798064896545996801         other_relevant_information  earthquake   \n",
       "1  797913886527602688                 caution_and_advice  earthquake   \n",
       "2  797867944546025472         other_relevant_information  earthquake   \n",
       "3  797958935126773760               sympathy_and_support  earthquake   \n",
       "4  797813020567056386  infrastructure_and_utility_damage  earthquake   \n",
       "\n",
       "  data_type                                         tweet_text  \n",
       "0     train  I feel a little uneasy about the idea of work ...  \n",
       "1     train  #eqnz Interislander ferry docking aborted afte...  \n",
       "2     train  Much of New Zealand felt the earthquake after ...  \n",
       "3     train  Noticing a lot of aftershocks on eqnz site, bu...  \n",
       "4     train  RT @E2NZ: Mike Clements, NZ police, says obvio...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = consolidated_disaster_tweet_data_df[consolidated_disaster_tweet_data_df[\"data_type\"]==\"train\"].reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "logical-aquarium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>class_label</th>\n",
       "      <th>event_type</th>\n",
       "      <th>data_type</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>798274825441538048</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>test</td>\n",
       "      <td>The earthquake in New Zealand was massive. Bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>798452064208568320</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>test</td>\n",
       "      <td>These pictures show the alarming extent of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>797804396767682560</td>\n",
       "      <td>sympathy_and_support</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>test</td>\n",
       "      <td>Just woke to news of another earthquake! WTF N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>798434862830993408</td>\n",
       "      <td>not_humanitarian</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>test</td>\n",
       "      <td>When theres an actual earthquake, landslide an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>797790705414377472</td>\n",
       "      <td>caution_and_advice</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>test</td>\n",
       "      <td>Tsunami warning for entire East Coast of NZ, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                        class_label  event_type  \\\n",
       "0  798274825441538048  infrastructure_and_utility_damage  earthquake   \n",
       "1  798452064208568320  infrastructure_and_utility_damage  earthquake   \n",
       "2  797804396767682560               sympathy_and_support  earthquake   \n",
       "3  798434862830993408                   not_humanitarian  earthquake   \n",
       "4  797790705414377472                 caution_and_advice  earthquake   \n",
       "\n",
       "  data_type                                         tweet_text  \n",
       "0      test  The earthquake in New Zealand was massive. Bil...  \n",
       "1      test  These pictures show the alarming extent of the...  \n",
       "2      test  Just woke to news of another earthquake! WTF N...  \n",
       "3      test  When theres an actual earthquake, landslide an...  \n",
       "4      test  Tsunami warning for entire East Coast of NZ, b...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = consolidated_disaster_tweet_data_df[consolidated_disaster_tweet_data_df[\"data_type\"]==\"test\"].reset_index(drop=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "improving-rotation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>class_label</th>\n",
       "      <th>event_type</th>\n",
       "      <th>data_type</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>798262465234542592</td>\n",
       "      <td>sympathy_and_support</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>dev</td>\n",
       "      <td>RT @MissEarth: New Zealand need our prayers af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>771464543796985856</td>\n",
       "      <td>caution_and_advice</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>dev</td>\n",
       "      <td>@johnaglass65 @gordonluke Ah, woke up to a nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>797835622471733248</td>\n",
       "      <td>requests_or_urgent_needs</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>dev</td>\n",
       "      <td>RT @terremotocentro: #eqnz if you need a tool ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>798021801540321280</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>dev</td>\n",
       "      <td>RT @BarristerNZ: My son (4) has drawn a pictur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>798727277794033664</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>dev</td>\n",
       "      <td>Due to earthquake damage our Defence Force is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                        class_label  event_type  \\\n",
       "0  798262465234542592               sympathy_and_support  earthquake   \n",
       "1  771464543796985856                 caution_and_advice  earthquake   \n",
       "2  797835622471733248           requests_or_urgent_needs  earthquake   \n",
       "3  798021801540321280         other_relevant_information  earthquake   \n",
       "4  798727277794033664  infrastructure_and_utility_damage  earthquake   \n",
       "\n",
       "  data_type                                         tweet_text  \n",
       "0       dev  RT @MissEarth: New Zealand need our prayers af...  \n",
       "1       dev  @johnaglass65 @gordonluke Ah, woke up to a nig...  \n",
       "2       dev  RT @terremotocentro: #eqnz if you need a tool ...  \n",
       "3       dev  RT @BarristerNZ: My son (4) has drawn a pictur...  \n",
       "4       dev  Due to earthquake damage our Defence Force is ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df = consolidated_disaster_tweet_data_df[consolidated_disaster_tweet_data_df[\"data_type\"]==\"dev\"].reset_index(drop=True)\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "combined-aggregate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_type</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hurricane</td>\n",
       "      <td>31674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flood</td>\n",
       "      <td>7815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fire</td>\n",
       "      <td>7792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>earthquake</td>\n",
       "      <td>6250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_type  Count\n",
       "3   hurricane  31674\n",
       "2       flood   7815\n",
       "1        fire   7792\n",
       "0  earthquake   6250"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby([\"event_type\"]).size().reset_index().rename(columns={0: \"Count\"}).sort_values(\"Count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "interpreted-swing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_label</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rescue_volunteering_or_donation_effort</td>\n",
       "      <td>14891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>8501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sympathy_and_support</td>\n",
       "      <td>6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>5715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>5110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>not_humanitarian</td>\n",
       "      <td>4407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caution_and_advice</td>\n",
       "      <td>3774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>displaced_people_and_evacuations</td>\n",
       "      <td>2800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>requests_or_urgent_needs</td>\n",
       "      <td>1833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>missing_or_found_people</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              class_label  Count\n",
       "8  rescue_volunteering_or_donation_effort  14891\n",
       "6              other_relevant_information   8501\n",
       "9                    sympathy_and_support   6250\n",
       "2       infrastructure_and_utility_damage   5715\n",
       "3                  injured_or_dead_people   5110\n",
       "5                        not_humanitarian   4407\n",
       "0                      caution_and_advice   3774\n",
       "1        displaced_people_and_evacuations   2800\n",
       "7                requests_or_urgent_needs   1833\n",
       "4                 missing_or_found_people    250"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby([\"class_label\"]).size().reset_index().rename(columns={0: \"Count\"}).sort_values(\"Count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "noted-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "RND_STATE = 2584\n",
    "train_df = train_df.sample(frac=1, random_state=RND_STATE).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-monster",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "geographic-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_subset(vectorizer, num_samples, model, train_df, model_type, semi_supervised=False, semi_supervised_iterations=1, warm_start=False):\n",
    "    # use this cell to reduce the train set to simulate a rapid labelling semi-supervised situation\n",
    "    training_df = train_df.loc[:num_samples]\n",
    "    #print(\"all records\",len(train_df))\n",
    "    #print(\"Training Records:\", len(training_df))\n",
    "    num_features = 'all'\n",
    "    target_column = \"event_type\" # \"class_label\" or \"event_type\"\n",
    "    X_train = vectorizer.transform(training_df[\"tweet_text\"])\n",
    "    X_test = vectorizer.transform(test_df[\"tweet_text\"])\n",
    "    y = training_df[target_column]\n",
    "    y_frac = training_df[target_column]\n",
    "    y_frac_index = y_frac.index\n",
    "    y_test = test_df[target_column]\n",
    "    model_start_time = datetime.now()\n",
    "    if warm_start:\n",
    "        try:\n",
    "            check_is_fitted(model)\n",
    "            model.partial_fit(X_train, y)\n",
    "        except:\n",
    "            model.fit(X_train, y)\n",
    "    else:\n",
    "        model.fit(X_train, y)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    if semi_supervised:\n",
    "        X_train = vectorizer.transform(train_df[\"tweet_text\"])\n",
    "        y = train_df[target_column]\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "        vectorizer.fit(train_df[\"tweet_text\"])\n",
    "        X_train = vectorizer.transform(train_df[\"tweet_text\"])\n",
    "        X_test = vectorizer.transform(test_df[\"tweet_text\"])\n",
    "        for r in range(semi_supervised_iterations):\n",
    "            y_train_pred[y_frac_index] = y_frac # where the labels are provided we use them, otherwise we use the predicted label for semi-supervised\n",
    "            if warm_start:\n",
    "                model.partial_fit(X_train, y_train_pred)\n",
    "            else:\n",
    "                model.fit(X_train, y_train_pred)\n",
    "            y_train_pred = model.predict(X_train)\n",
    "    \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    model_end_time = datetime.now()\n",
    "    # Time taken on the dummy is not part of the main model time\n",
    "    dummy_model = DummyClassifier(strategy=\"stratified\", random_state=RANDOM_STATE)\n",
    "    dummy_model.fit(X_train, y)\n",
    "    y_train_pred_dummy = dummy_model.predict(X_train)\n",
    "    y_test_pred_dummy = dummy_model.predict(X_test)\n",
    "    run_time = (model_end_time - model_start_time).total_seconds()\n",
    "    \n",
    "    results = {}\n",
    "    results['model_type'] = model_type\n",
    "    results['vectorizer_num_features'] = vectorizer.__dict__['max_features']\n",
    "    results['semi_supervised'] = semi_supervised\n",
    "    results['samples'] = num_samples\n",
    "    results['dummy_train_accuracy'] = accuracy_score(y, y_train_pred_dummy)\n",
    "    results['dummy_test_accuracy'] = accuracy_score(y_test, y_test_pred_dummy)\n",
    "    results['train_accuracy'] = accuracy_score(y, y_train_pred)\n",
    "    results['test_accuracy'] = accuracy_score(y_test, y_test_pred)\n",
    "    results['run_time'] = run_time\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fluid-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.themes.enable('fivethirtyeight')\n",
    "def chart_results_curve(df, model_type):\n",
    "    sel_multi = alt.selection_multi(fields=['vectorizer_num_features'])\n",
    "\n",
    "    color = alt.condition(sel_multi,\n",
    "                      alt.Color('vectorizer_num_features:N'),\n",
    "                      alt.value('lightgray'))\n",
    "\n",
    "    #title = \"Baseline Accuracy on Test Set by Number of Samples: \" + str(model_type)\n",
    "    title = str(model_type)\n",
    "    chrt_super = alt.Chart(df, title=title).mark_line().encode(\n",
    "        x=alt.X('samples:Q', axis=alt.Axis(grid=False, titleFontSize=14, title='Number of Training Labels Used')),\n",
    "        y=alt.Y('test_accuracy:Q', axis=alt.Axis(grid=False, titleFontSize=14, title='Accuracy on Test Set'), scale=alt.Scale(domain=[0.25, .8])),\n",
    "        color=color,\n",
    "        tooltip=[alt.Tooltip(\"samples\", format=\",.0f\"), alt.Tooltip(\"test_accuracy\", format=\",.4f\"), \n",
    "                 'vectorizer_num_features', alt.Tooltip(\"run_time\", format=\",.4f\")]\n",
    "    ).properties(\n",
    "        width=240,\n",
    "        height=320\n",
    "    ).add_selection(\n",
    "    sel_multi\n",
    "    ) \n",
    "    \n",
    "    #     legend = alt.Chart(df).mark_point().encode(\n",
    "    #         y=alt.Y('vectorizer_num_features:N', axis=alt.Axis(orient='right')),\n",
    "    #         color=color\n",
    "    #     ).add_selection(\n",
    "    #         sel_multi\n",
    "    #     )    \n",
    "\n",
    "    #chrt_super = chrt_super | legend\n",
    "    \n",
    "    return chrt_super   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "frozen-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart_accuracy_speed_scatter(df, model_type, chart_upper_limit):\n",
    "    #title = \"Baseline Accuracy on Test Set by Number of Samples: \" + str(model_type)\n",
    "    title = str(model_type)\n",
    "    chrt_super = alt.Chart(df, title=title).mark_circle().encode(\n",
    "        x=alt.X('run_time:Q', axis=alt.Axis(grid=False, titleFontSize=14, title='Run Time in Seconds'), scale=alt.Scale(domain=[0., chart_upper_limit])),\n",
    "        y=alt.Y('test_accuracy:Q', axis=alt.Axis(grid=False, titleFontSize=14, title='Accuracy on Test Set'), scale=alt.Scale(domain=[0.25, .8])),\n",
    "        color=alt.Color('model_type:N', title=\"Model Type\"),\n",
    "        tooltip=['model_type', alt.Tooltip(\"samples\", format=\",.0f\"), alt.Tooltip(\"test_accuracy\", format=\",.4f\"), \n",
    "                 'vectorizer_num_features:N', alt.Tooltip(\"run_time\", format=\",.4f\")]\n",
    "    ).properties(\n",
    "        width=240,\n",
    "        height=200\n",
    "    )\n",
    "    \n",
    "    return chrt_super   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "classical-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_sgd(use_warm_start):\n",
    "    model = SGDClassifier(loss=\"modified_huber\", max_iter=1000, tol=1e-3, random_state=2584, n_jobs=-1, warm_start=use_warm_start)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-pillow",
   "metadata": {},
   "source": [
    "### Prepare for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "suited-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the checkpoints for the list of number of labels against which we check the model accuracy on the test set\n",
    "upper_limit = len(train_df)\n",
    "step_size = 1000\n",
    "label_count_checkpoints = [i for i in range(0, upper_limit, step_size)]\n",
    "label_count_checkpoints.pop(0)\n",
    "label_count_checkpoints = [250, 500, 750] + label_count_checkpoints\n",
    "if upper_limit!=label_count_checkpoints[-1]: label_count_checkpoints.append(upper_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "norwegian-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "semi_supervised_iterations = 1\n",
    "run_semi_supervised = True\n",
    "use_warm_start = False #True\n",
    "\n",
    "tfidf_max_features = [100, 200, 300, 500, 800, None]\n",
    "\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "# Define Models\n",
    "model_dict = {}\n",
    "# model_dict['GaussianProcessClassifier'] = GaussianProcessClassifier(kernel=kernel,random_state=0)\n",
    "#model_dict['MultinomialNB']= MultinomialNB()\n",
    "model_dict['LinearSVC'] = LinearSVC(random_state=RANDOM_STATE)\n",
    "#model_dict['SGDClassifier'] = initiate_sgd(use_warm_start)\n",
    "#model_dict['Perceptron'] = Perceptron(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "#model_dict['PassiveAggressiveClassifier'] = PassiveAggressiveClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "# estimators = [\n",
    "#     ('MultinomialNB', MultinomialNB()),\n",
    "#     ('LinearSVC', LinearSVC(random_state=RANDOM_STATE)),\n",
    "#     ('SGDClassifier', initiate_sgd(use_warm_start)),\n",
    "#     ('Perceptron', Perceptron(random_state=RANDOM_STATE, n_jobs=-1)),\n",
    "#     ('PassiveAggressiveClassifier', PassiveAggressiveClassifier(random_state=RANDOM_STATE, n_jobs=-1)),\n",
    "# ]\n",
    "#model_dict['StackingClassifier'] = StackingClassifier(estimators=estimators, final_estimator=LinearSVC(random_state=RANDOM_STATE), n_jobs=-1)\n",
    "\n",
    "# Baseline Model\n",
    "#model = LinearSVC(random_state=RANDOM_STATE)\n",
    "# App Model\n",
    "model = initiate_sgd(use_warm_start)\n",
    "\n",
    "df_results = pd.DataFrame(columns = ['model_type', 'vectorizer_num_features', 'semi_supervised', 'samples', 'dummy_train_accuracy', \n",
    "                                     'dummy_test_accuracy', 'train_accuracy', 'test_accuracy', 'run_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "vital-interview",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [09:29<00:00, 94.90s/it]\n"
     ]
    }
   ],
   "source": [
    "# Supervised\n",
    "for tmf in tqdm(tfidf_max_features):\n",
    "    # Vectorize the train data - we have a corpus before we start labeling\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words=\"english\", max_features=tmf)\n",
    "    vectorizer.fit(train_df[\"tweet_text\"])\n",
    "    for model_type, model in model_dict.items():\n",
    "        for current_num_samples in label_count_checkpoints:\n",
    "            results = supervised_subset(vectorizer, current_num_samples, model, train_df, model_type, warm_start=use_warm_start)\n",
    "            df_results = df_results.append(results, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "devoted-thumbnail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>vectorizer_num_features</th>\n",
       "      <th>semi_supervised</th>\n",
       "      <th>samples</th>\n",
       "      <th>dummy_train_accuracy</th>\n",
       "      <th>dummy_test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>run_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>50000</td>\n",
       "      <td>0.407392</td>\n",
       "      <td>0.401319</td>\n",
       "      <td>0.999860</td>\n",
       "      <td>0.973813</td>\n",
       "      <td>1.453005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>51000</td>\n",
       "      <td>0.404639</td>\n",
       "      <td>0.401517</td>\n",
       "      <td>0.999882</td>\n",
       "      <td>0.973879</td>\n",
       "      <td>1.452001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>52000</td>\n",
       "      <td>0.406050</td>\n",
       "      <td>0.401055</td>\n",
       "      <td>0.999865</td>\n",
       "      <td>0.974340</td>\n",
       "      <td>1.396995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>53000</td>\n",
       "      <td>0.405690</td>\n",
       "      <td>0.401385</td>\n",
       "      <td>0.999849</td>\n",
       "      <td>0.974274</td>\n",
       "      <td>1.453985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>53531</td>\n",
       "      <td>0.403803</td>\n",
       "      <td>0.401055</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.974340</td>\n",
       "      <td>1.418003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_type vectorizer_num_features semi_supervised samples  \\\n",
       "337  LinearSVC                    None           False   50000   \n",
       "338  LinearSVC                    None           False   51000   \n",
       "339  LinearSVC                    None           False   52000   \n",
       "340  LinearSVC                    None           False   53000   \n",
       "341  LinearSVC                    None           False   53531   \n",
       "\n",
       "     dummy_train_accuracy  dummy_test_accuracy  train_accuracy  test_accuracy  \\\n",
       "337              0.407392             0.401319        0.999860       0.973813   \n",
       "338              0.404639             0.401517        0.999882       0.973879   \n",
       "339              0.406050             0.401055        0.999865       0.974340   \n",
       "340              0.405690             0.401385        0.999849       0.974274   \n",
       "341              0.403803             0.401055        0.999851       0.974340   \n",
       "\n",
       "     run_time  \n",
       "337  1.453005  \n",
       "338  1.452001  \n",
       "339  1.396995  \n",
       "340  1.453985  \n",
       "341  1.418003  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see results on the full train data\n",
    "df_results.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "minimal-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"model_accuracy_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-earth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▎                                                                              | 3/57 [00:27<08:24,  9.34s/it]"
     ]
    }
   ],
   "source": [
    "# Semi Supervised with Same Model\n",
    "if run_semi_supervised:\n",
    "    if use_warm_start:\n",
    "        model = initiate_sgd_warm_start()\n",
    "    for current_num_samples in tqdm(label_count_checkpoints):\n",
    "        results = supervised_subset(vectorizer, current_num_samples, model, train_df, model_type, warm_start=use_warm_start,\n",
    "                                    semi_supervised=True, semi_supervised_iterations=semi_supervised_iterations)\n",
    "        df_results = df_results.append(results, ignore_index=True)\n",
    "    df_results.to_csv(\"model_accuracy_results_semi_supervised.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.loc[df_results['samples']<=1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.loc[df_results['vectorizer_num_features']==800]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-agriculture",
   "metadata": {},
   "source": [
    "## Results Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-corpus",
   "metadata": {},
   "source": [
    "### Visualising Results Curve - Accuracy vs Number of Training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "satisfied-produce",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-efa71ed677c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mchrts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchart_results_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrow1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchrts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mchrts\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mchrts\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mrow2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchrts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mchrts\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mchrts\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "chrts = []\n",
    "for model_type in model_dict.keys():\n",
    "    chrts.append(chart_results_curve(df_results.loc[df_results['model_type']==model_type], model_type))\n",
    "row1 = alt.hconcat(chrts[0] | chrts [1] | chrts [2] )\n",
    "row2 = alt.hconcat(chrts[3] | chrts [4] | chrts [5] )\n",
    "\n",
    "super_chrt = alt.vconcat(row1, row2).properties(\n",
    "    title='Baseline Accuracy Curves with True Labels'\n",
    ").configure_title(\n",
    "    fontSize=20,\n",
    "    anchor='start',\n",
    "    color='gray'\n",
    ")\n",
    "super_chrt.save('super_chrt_harder_target.html')\n",
    "super_chrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-physiology",
   "metadata": {},
   "source": [
    "Zoom in to the early part of the chart and the first labels added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_early_sample_limit = 3000\n",
    "chrts_early = []\n",
    "for model_type in model_dict.keys():\n",
    "    chrts_early.append(chart_results_curve(df_results.loc[(df_results['model_type']==model_type) & \n",
    "                                                          (df_results['samples']<=upper_early_sample_limit)], model_type))\n",
    "    \n",
    "\n",
    "row1 = alt.hconcat(chrts_early[0] | chrts_early [1] | chrts_early [2] )\n",
    "row2 = alt.hconcat(chrts_early[3] | chrts_early [4] | chrts_early [5] )\n",
    "\n",
    "super_chrt_early = alt.vconcat(row1, row2).properties(\n",
    "    title='Baseline Accuracy Curves with True Labels - Few Labels'\n",
    ").configure_title(\n",
    "    fontSize=20,\n",
    "    anchor='start',\n",
    "    color='gray'\n",
    ")\n",
    "super_chrt_early.save('super_chrt_early_harder_target.html')\n",
    "super_chrt_early"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-xerox",
   "metadata": {},
   "source": [
    "### Visualizing Accuracy vs Speed in App for Recommended Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrts = []\n",
    "chart_upper_limit = 4.5\n",
    "for tmf in tfidf_max_features:\n",
    "    if tmf is None:\n",
    "        chrts.append(chart_accuracy_speed_scatter(df_results.loc[(df_results['vectorizer_num_features'].isna()) & (df_results['model_type']!='StackingClassifier')], tmf, chart_upper_limit))\n",
    "    else:\n",
    "        chrts.append(chart_accuracy_speed_scatter(df_results.loc[(df_results['vectorizer_num_features']==tmf) & (df_results['model_type']!='StackingClassifier')], tmf, chart_upper_limit))\n",
    "row_chrt1 = alt.hconcat(chrts[0] | chrts [1] | chrts [2])\n",
    "row_chrt2 = alt.hconcat(chrts[3] | chrts [4] | chrts [5])\n",
    "super_chrt_speed_accuracy = alt.vconcat(row_chrt1, row_chrt2).properties(\n",
    "    title='Baseline Accuracy to Speed with True Labels'\n",
    ").configure_title(\n",
    "    fontSize=20,\n",
    "    anchor='start',\n",
    "    color='gray'\n",
    ")\n",
    "super_chrt_speed_accuracy.save('super_chrt_speed_accuracy_harder_target.html')\n",
    "super_chrt_speed_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrts = []\n",
    "chart_upper_limit = 250\n",
    "for tmf in tfidf_max_features:\n",
    "    if tmf is None:\n",
    "        chrts.append(chart_accuracy_speed_scatter(df_results.loc[(df_results['vectorizer_num_features'].isna()) & (df_results['model_type']=='StackingClassifier')], tmf, chart_upper_limit))\n",
    "    else:\n",
    "        chrts.append(chart_accuracy_speed_scatter(df_results.loc[(df_results['vectorizer_num_features']==tmf) & (df_results['model_type']=='StackingClassifier')], tmf, chart_upper_limit))\n",
    "row_chrt1 = alt.hconcat(chrts[0] | chrts [1] | chrts [2])\n",
    "row_chrt2 = alt.hconcat(chrts[3] | chrts [4] | chrts [5])\n",
    "super_chrt_speed_accuracy = alt.vconcat(row_chrt1, row_chrt2).properties(\n",
    "    title='Baseline Accuracy to Speed with True Labels'\n",
    ").configure_title(\n",
    "    fontSize=20,\n",
    "    anchor='start',\n",
    "    color='gray'\n",
    ")\n",
    "super_chrt_speed_accuracy.save('super_chrt_speed_accuracy_harder_target_stacking.html')\n",
    "super_chrt_speed_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "end_time.strftime(\"%Y/%m/%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = end_time - start_time\n",
    "print(\"duration :\", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-graduate",
   "metadata": {},
   "source": [
    "# To Do\n",
    "* Viz Small Multiples scatter plot accuracy to speed with color for num features and a plot each for model type\n",
    "* Save a Version of this Notebook as Baseline\n",
    "* Run a New Version of this Notebook with Carlo's SGD warm start incremental model - output results and charts\n",
    "* Compare Accuracy and Run Times between these 2 baselines\n",
    "* Chart Baselines Against Each other and compare run times to create the baseline.\n",
    "* Ensemble\n",
    "* Semi Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-preparation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
